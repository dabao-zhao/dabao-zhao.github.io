<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>赵大宝的博客</title>
    <link>https://dabao-zhao.github.io/</link>
    <description>Recent content on 赵大宝的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 16 Aug 2023 15:31:25 +0800</lastBuildDate><atom:link href="https://dabao-zhao.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>跳数索引：后起新秀ClickHouse</title>
      <link>https://dabao-zhao.github.io/posts/%E8%B7%B3%E6%95%B0%E7%B4%A2%E5%BC%95%E5%90%8E%E8%B5%B7%E6%96%B0%E7%A7%80clickhouse/</link>
      <pubDate>Wed, 16 Aug 2023 15:31:25 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E8%B7%B3%E6%95%B0%E7%B4%A2%E5%BC%95%E5%90%8E%E8%B5%B7%E6%96%B0%E7%A7%80clickhouse/</guid>
      <description>并行能力 CPU 吞吐和性能 ClickHouse 会充分利用多核，对本地大量数据做快速的计算，所以一个请求就可以用光所有系统资源，官方建议 ClickHouse 的查询 QPS 限制在 100 左右。
ClickHouse 的场景如果是对用户服务的，最好对这种查询做缓存。而且，界面在加载时要设置 30 秒以上的等待时间，因为我们的请求可能在排队等待别的查询。
推荐的优化思路是：基于排序字段做范围查询过滤后，再做聚合查询。高并发查询数据的服务和缓慢查询的服务需要隔离开。
批量写入优化 批量插入数据时，客户端会将要插入的数据先在本地缓存一段时间，直到积累足够配置的 block_size 后才会把数据批量提交到服务端，以此提高写入的性能。
除了客户端做的合并，ClickHouse 的引擎 MergeTree 也做了类似的工作。MergeTree 采用了批量写入磁盘、定期合并方式（batch write-merge）。
当我们批量输入的数据量小于 min_bytes_for_wide_part 设置时，会按 compact part 方式落盘。这种方式会将落盘的数据放到一个 data.bin 文件中， merge 时会有很好的写效率，这种方式适合于小量业务数据筛选使用。
当我们批量输入的数据量大于 min_bytes_for_wide_part 设置时，会按 wide part 方式落盘，落盘数据的时候会按字段生成不同的文件。这个方式适用于字 段较多的数据，merge 相对会慢一些，但是对于指定参与计算列的统计计算，并行吞吐写入和计算能力会更强，适合分析指定小范围的列计算。
查询效率 Clickhouse 有两种索引方式，一种是主键索引，这个是在建表时就需要指定的；另一种是跳表索引，用来跳过一些数据。
ClickHouse 的查询工作流程：
根据查询条件，查询过滤出要查询需要读取的 data part 文件夹范围； 根据 data part 内数据的主键索引、过滤出要查询的 granule； 使用跳表索引跳过不符合的 granule； 范围内数据进行计算、汇总、统计、筛选、排序； 返回结果。 只有第四步里的几个操作是并行的
很难做索引查询优化的原因：
大部分数据的特征不是很明显、建立的索引区分度不够 目录过多，有多份数据同时散落在多个 data parts 文件夹内，需要加载所有 date part 的索引挨个查询 其他特性 物化视图 物化视图会将数据源的数据通过聚合函数实时统计计算，每次我们查询这个表，就能获得表规定的统计结果</description>
    </item>
    
    <item>
      <title>实时统计：链路跟踪实时计算中的实用算法</title>
      <link>https://dabao-zhao.github.io/posts/%E5%AE%9E%E6%97%B6%E7%BB%9F%E8%AE%A1%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%94%A8%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 16 Aug 2023 14:27:45 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%AE%9E%E6%97%B6%E7%BB%9F%E8%AE%A1%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%94%A8%E7%AE%97%E6%B3%95/</guid>
      <description>URL 去参数聚合 主要是 RESTful 的接口需要做处理：
同一个 API 由于不同的参数无法归类 同一网址不同的 method 操作在 RESTful 中实际也是不同的实现 人工配置替换模版 也就是人工配置出一个 URL 规则，用来筛选出符合规则的日志并替换掉关键部分的参数。
类Radix tree效果： /user - /* - - /info - - - :GET - - - :PUT - - /friend - - - /* (代表这里替换) - - - - :DELETE 具体实现是将网址通过 / 进行分割，逐级在前缀搜索树查找
数据特征筛选 //根据数据特征过滤网址内参数 function filterUrl($url) { $urlArr = explode(&amp;#34;/&amp;#34;, $url); foreach ($urlArr as $urlIndex =&amp;gt; $urlItem) { $totalChar = 0; //有多少字母 $totalNum = 0; //有多少数值 $totalLen = strlen($urlItem); //总长度 for ($index = 0; $index &amp;lt; $totalLen; $index++) { if (is_numeric($urlItem[$index])) { $totalNum++; } else { $totalChar++; } } //过滤md5 长度32或64 内容有数字 有字符混合 直接认为是md5 if (($totalLen == 32 || $totalLen == 64) &amp;amp;&amp;amp; $totalChar &amp;gt; 0 &amp;amp;&amp;amp; $totalNum &amp;gt;0) { $urlArr[$urlIndex] = &amp;#34;*md*&amp;#34;; continue; } //字符串 data 参数是数字和英文混合 长度超过3(回避v1/v2一类版本) if ($totalLen &amp;gt; 3 &amp;amp;&amp;amp; $totalChar &amp;gt; 0 &amp;amp;&amp;amp; $totalNum &amp;gt; 0) { $urlArr[$urlIndex] = &amp;#34;*data*&amp;#34;; continue; } //全是数字在网址中认为是id一类， 直接进行替换 if ($totalChar == 0 &amp;amp;&amp;amp; $totalNum &amp;gt; 0) { $urlArr[$urlIndex] = &amp;#34;*num*&amp;#34;; continue; } } return implode(&amp;#34;/&amp;#34;, $urlArr); } 时间分块统计 每个数据统计块内聚合了以下指标：</description>
    </item>
    
    <item>
      <title>引擎分片：Elasticsearch如何实现大数据检索</title>
      <link>https://dabao-zhao.github.io/posts/%E5%BC%95%E6%93%8E%E5%88%86%E7%89%87elasticsearch%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2/</link>
      <pubDate>Wed, 16 Aug 2023 10:24:58 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%BC%95%E6%93%8E%E5%88%86%E7%89%87elasticsearch%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2/</guid>
      <description>Elasticsearch 架构 Master 节点：负责集群内调度决策，集群状态、节点信息、索引映射、分片信息、路由信息，Master 真正主节点是通过选举诞生的，一般一个集群内至少要有三个 Master 可竞选成员，防止主节点损坏； Data 存储节点：用于存储数据及计算，分片的主从副本，热点节点，冷数据节点； Client 协调节点：协调多个副本数据查询服务，聚合各个副本的返回结果，返回给客户端； Kibana 计算节点：作用是实时统计分析、聚合分析统计数据、图形聚合展示。 Elasticsearch 索引 Elasticsearch 其底层全文检索使用的是 Lucene 引擎，但 Lucene 不支持分布式，Elasticsearch 的分布式功能是通过基础分片来实现的。
索引通过倒排索引实现。 Elasticsearch 集群的索引保存在 Lucene 的 segment 文件中，segment 主要保存了三种索引：
Term Index（单词词典索引）：用于关键词（Term）快速搜索，Term index 是基础 Trie 树改进的 FST（Finite State Transducer 有限状态传感器，占用内存少） 实现的二级索引。平时这个树会放在内存中，用于减少磁盘 IO 加快 Term 查找速度，检索时会通过 FST 快速找到 Term Dictionary 对应的词典文件 block。 Term Dictionary（单词词典）：单词词典索引中保存的是单词（Term）与 Posting List 的关系，而这个单词词典数据会按 block 在磁盘中排序压缩保存，相比 B-Tree 更节省空间，其中保存了单词的前缀后缀，可以用于近似词及相似词查询，通过这个词典可以找到相关的倒排索引列表位置。 Posting List（倒排列表）：倒排列表记录了关键词 Term 出现的文档 ID，以及其所在文档中位置、偏移、词频信息，这是我们查找的最终文档列表，我们拿到这些就可以拿去排序合并了。 Elasticsearch 写入 在数据入库之前，会先进行分词，过滤掉无用符号等分隔词，找出文档中每个关键词（Term）在文档中的位置及频率权重；然后，将这些关键词保存在 Term Index 以及 Term Dictionary 内；最后，将每个关键词对应的文档 ID 和权重、位置等信息排序合并到 Posting List 中进行保存。</description>
    </item>
    
    <item>
      <title>链路追踪：如何定制一个分布式链路跟踪系统</title>
      <link>https://dabao-zhao.github.io/posts/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E5%A6%82%E4%BD%95%E5%AE%9A%E5%88%B6%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Tue, 15 Aug 2023 15:50:03 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E5%A6%82%E4%BD%95%E5%AE%9A%E5%88%B6%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F/</guid>
      <description>Metrics、Tracing 和 Logging 常见监控系统主要有三种类型：Metrics、Tracing 和 Logging。
常见的开源 Metrics 有 Zabbix、Nagios、Prometheus、InfluxDb、OpenFalcon。主要做各种量化指标汇总统计，比如监控系统的容量剩余、每秒请求量、平均响应速度、某个时段请求量多少。
常见的开源链路跟踪有 Jaeger、Zipkin、Pinpoint、Skywalking。主要是通过分析每次请求链路监控分析的系统，可以通过 TraceID 查找一次请求的依赖及调用链路，分析故障点和传导过程的耗时。
常见的开源 Logging 有 ELK、Loki、Loggly。主要是对文本日志的收集归类整理，可以对错误日志进行汇总、警告，并分析系统错误异常等情况。
三种监控体系最大的区别就是日志格式标准。
OpenTelemetry 标准 主要是对数据采集和标准规范的统一，其他的不参与。
TODO 后面出个介绍
分布式链路跟踪系统 主要功能服务：
监控日志标准 埋点 SDK（AOP 或侵入式） 日志收集 分布式日志传输 分布式日志存储 分布式检索计算 分布式实时分析 个性化定制指标盘 系统警告 ELK 即可实现以上功能：
日志收集（Filebeat） 日志传输（Kafka+Logstash） 日志存储（Elasticsearch） 检索计算（Elasticsearch + Kibana） 实时分析（Kibana） 个性定制表格查询（Kibana） 这样之后只需要制定日志格式和埋点 SDK 即可实现一个具有分布式链路跟踪、 Metrics、日志分析的系统。 TODO 后面出个介绍
原理 在请求发起方发送请求时或服务被请求时生成一个 UUID（TraceID），被请求期间的业务产生的任何日志（Warning、Info、Debug、Error）、任何依赖资源 请求（MySQL、Kafka、Redis）、任何内部接口调用（Restful、Http、RPC）都会带上这个 UUID。 这样，当我们把所有拥有同样 UUID 的日志收集起来时，就可以根据时间（有误差）、RPCID 或 SpanID，将它们按依赖请求顺序串起来。 只要日志足够详细，我们就能监控到系统大部分的工作状态，比如用户请求一个服务会调用多少个接口，每个数据查询的 SQL 以及具体耗时调用的内网请求参数是什么、 调用的内网请求返回是什么、内网被请求的接口又做了哪些操作、产生了哪些异常信息等等。 同时，我们可以通过对这些日志做归类分析，分析项目之间的调用关系、项目整体健康程度、 对链路深挖自动识别出故障点等，帮助我们主动、快速地查找问题。
TraceID、RPCID、SpanID TraceID 用于标识某一次具体的请求 ID。 SpanID 用于标识单个服务中的一个操作。 RPCID 用于标识操作在整个调用链路中的位置。</description>
    </item>
    
    <item>
      <title>稀疏索引：为什么高并发写不推荐关系数据库</title>
      <link>https://dabao-zhao.github.io/posts/%E7%A8%80%E7%96%8F%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E9%AB%98%E5%B9%B6%E5%8F%91%E5%86%99%E4%B8%8D%E6%8E%A8%E8%8D%90%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Tue, 15 Aug 2023 09:51:35 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%A8%80%E7%96%8F%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E9%AB%98%E5%B9%B6%E5%8F%91%E5%86%99%E4%B8%8D%E6%8E%A8%E8%8D%90%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>B+Tree MySQL 几乎所有查询都是通过索引去缩小扫描数据的范围，然后再回到表中对范围内数据进行遍历加工、过滤，最终拿到我们的业务需要的数据。
B+Tree 的特点在于只有在最底层才会存储真正的数据 ID，通过这个 ID 就可以提取到数据的具体内容，同时 B+Tree 索引最底层的数据是按索引字段顺序进行存储的。 通过这种设计方式，我们只需进行 1～3 次 IO（树深度决定了 IO 次数）就能找到所查范围内序好的数据，而树形的索引最影响查询效率的是树的深度以及数据量。
MySQL 的索引是使用 Page 作为单位进行存储的，而每页只能存储 16KB（innodb_page_size）数据。如果我们每行数据的索引是1KB，那么除去 Page 页的一些固定 结构占用外，一页只能放 16 条数据，这导致树的一些分支装不下更多数据时，就需要对索引的深度再加一层。就可以推导出：索引第一层放 16 条，树第二层大概能放 2 万条， 树第三层大概能放 2400 万条，三层的深度 B+Tree 按主键查找数据每次查询需要 3 次 IO（一层索引在内存，IO 两次索引，最后一次是拿数据），这也是一些文章会推荐说 MySQL 的单表记录最好不要超过 2000 万条的原因。
不过不是绝对的，和每行数据的大小有关系
B+Tree 其实有些离题了……但是硬说也能原回来，因为索引太多也是会影响写入速度的
文中最后总结不适合的原因是：主从的部署模式，大批量的插入会造成主库响应缓慢、主从同步延迟增大等问题
稀疏索引 LSM Tree TODO 后面再找个教程
主要以 RocksDB 为例介绍。
RocksDB LSM 新数据写入时会在内存中暂存，当内存中数据积累到一定程度后，会将内存中数据和索引做顺序写，落地形成一个数据块。新生成的数据块会保存在 Level 0 层， 每一层的数据块和数据量超过一定程度时，RocksDB 合并不同 Level 的数据，将多个数据块内的数据和索引合并在一起，并推送到 Level 的下一层。
当我们查询一个 key 的时候，RocksDB 会先查内存。如果没找到，会从 Level 0 层到下层，每层按生成最新到最老的顺序去查询每层的数据块。同时为了减少 IO 次数，每个数据块 都会有一个 BloomFilter 辅助索引，来辅助确认这个数据块中是否可能有对应的 Key；如果当前数据块没有，那么可以快速去找下一个数据块，直到找到为止。</description>
    </item>
    
    <item>
      <title>分布式事务：多服务的2PC、TCC都是怎么实现的</title>
      <link>https://dabao-zhao.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%A4%9A%E6%9C%8D%E5%8A%A1%E7%9A%842pctcc%E9%83%BD%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/</link>
      <pubDate>Mon, 14 Aug 2023 14:51:34 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%A4%9A%E6%9C%8D%E5%8A%A1%E7%9A%842pctcc%E9%83%BD%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/</guid>
      <description>XA 协议 三个角色 应用（AP） 应用是具体的业务逻辑代码实现，业务逻辑通过请求事务协调器开启全局事务，在事务协调器注册多个子事务后，业务代码会依次给所有参与事务的子业务下发请求。 待所有子业务提交成功后，业务代码根据返回情况告诉事务协调器各个子事务的执行情况，由事务协调器决策子事务是提交还是回滚（有些实现是事务协调器发请求给子服务）。
事务协调器（TM） 用于创建主事务，同时协调各个子事务。事务协调器会根据各个子事务的执行情况，决策这些子事务最终是提交执行结果，还是回滚执行结果。此外，事务协调器很多时候还会自动帮我们提交事务；
资源管理器（RM） 是一种支持事务或 XA 协议的数据资源，比如 MySQL、Redis 等
两个阶段 Prepare 阶段 在 Prepare 阶段，事务协调器会通过 xid（事务唯一标识，由业务或事务协调器生成）协调多个资源管理器执行子事务，所有子事务执行成功后会向事务协调器汇报。 这时的子事务执行成功是指事务内 SQL 执行成功，并没有执行事务的最终 commit（提交），所有子事务是提交还是回滚，需要等事务协调器做最终决策。
Commit 阶段 当事务协调器收到所有资源管理器成功执行子事务的消息后，会记录事务执行成功，并对子事务做真正提交。如果 Prepare 阶段有子事务失败，或者事务协调器在一段 时间内没有收到所有子事务执行成功的消息，就会通知所有资源管理器对子事务执行回滚的操作。
事务的几个状态 ACTIVE：子事务 SQL 正在执行中； IDLE：子事务执行完毕等待切换 Prepared 状态，如果本次操作不参与回滚，就可以直接提交完成； PREPARED：子事务执行完毕，等待其他服务实例的子事务全部 Ready。 COMMITED/FAILED：所有子事务执行成功 / 失败后，一起提交或回滚。 2PC 缺点：
同步阻塞问题：事务的执行过程中，所有参与事务的节点都会对其占用的公共资源加锁，导致其他访问公共资源的进程或者线程阻塞。 单点故障问题：如果事务管理器发生故障，则资源管理器会一直阻塞。 数据不一致问题：如果在 Commit 阶段，由于网络或者部分资源管理器发生故障，导致部分资源管理器没有接收到事务管理器发送过来的 Commit 消息，会引起数据不一致的问题。 无法解决的问题：如果在 Commit 阶段，事务管理器发出 Commit 消息后宕机，并且唯一接收到这条 Commit 消息的资源管理器也宕机了，则无法确认事务是否已经提交。 3PC 3PC 模型把 2PC 模型中的 Prepare 阶段一分为二，最终形成 3 个阶段：
CanCommit 阶段：为了减少因等待锁定数据导致的超时情况，提高事务成功率，事务协调器会发送消息确认资源管理器的资源锁定情况，以及所有子事务的数据库锁定数据的情况。 PreCommit 阶段：执行 2PC 的 Prepare 阶段； DoCommit 阶段：执行 2PC 的 Commit 阶段。 缺点：</description>
    </item>
    
    <item>
      <title>系统隔离：如何应对高并发流量冲击</title>
      <link>https://dabao-zhao.github.io/posts/%E7%B3%BB%E7%BB%9F%E9%9A%94%E7%A6%BB%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E9%AB%98%E5%B9%B6%E5%8F%91%E6%B5%81%E9%87%8F%E5%86%B2%E5%87%BB/</link>
      <pubDate>Mon, 14 Aug 2023 14:07:19 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%B3%BB%E7%BB%9F%E9%9A%94%E7%A6%BB%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E9%AB%98%E5%B9%B6%E5%8F%91%E6%B5%81%E9%87%8F%E5%86%B2%E5%87%BB/</guid>
      <description>感觉又是水的一篇
首先是部署的隔离，内网和外网部署要物理隔离 然后是网关的隔离，内网网关和外网网关隔离，流量经过外网网关、外网、内网网关才能到内网。其中外网网关和内网网关都可以做限流和熔断。外网服务要保证内网 网关断开后，仍旧能正常独立运转一小时以上。 然后是减少外网和内网的 api 互动，内外通过缓存来交互数据，但要保证数据同步是单向的。如果内网想要修改数据可以通过请求外网接口，外网对内网的数据同步可以通过消息队列。 然后是讲了消息队列的优点：
队列拥有良好吞吐并且能够动态扩容，可应对各种流量冲击场景； 可通过动态控制内网消费线程数，从而实现内网流量可控； 内网消费服务在高峰期可以暂时离线，内网服务可以临时做一些停机升级操作； 内网服务如果出现 bug，导致消费数据丢失，可以对队列消息进行回放实现重新消费； Kafka 是分区消息同步，消息是顺序的，很少会乱序，可以帮我们实现顺序同步； 消息内容可以保存很久，加入 TraceID 后查找方便并且透明，利于排查各种问题。 感觉水的一部分原因是太理论了，和实战没啥关系……</description>
    </item>
    
    <item>
      <title>强一致锁：如何解决高并发下的库存争抢问题</title>
      <link>https://dabao-zhao.github.io/posts/%E5%BC%BA%E4%B8%80%E8%87%B4%E9%94%81%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E7%9A%84%E5%BA%93%E5%AD%98%E4%BA%89%E6%8A%A2%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 14 Aug 2023 11:11:17 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%BC%BA%E4%B8%80%E8%87%B4%E9%94%81%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E7%9A%84%E5%BA%93%E5%AD%98%E4%BA%89%E6%8A%A2%E9%97%AE%E9%A2%98/</guid>
      <description></description>
    </item>
    
    <item>
      <title>领域拆分：如何合理地拆分系统</title>
      <link>https://dabao-zhao.github.io/posts/%E9%A2%86%E5%9F%9F%E6%8B%86%E5%88%86%E5%A6%82%E4%BD%95%E5%90%88%E7%90%86%E5%9C%B0%E6%8B%86%E5%88%86%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Thu, 10 Aug 2023 17:10:18 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E9%A2%86%E5%9F%9F%E6%8B%86%E5%88%86%E5%A6%82%E4%BD%95%E5%90%88%E7%90%86%E5%9C%B0%E6%8B%86%E5%88%86%E7%B3%BB%E7%BB%9F/</guid>
      <description>以供货协商流程为例讲述了如何拆分系统。
首先进行流程分析，要多和产品、研发团队沟通，以确定主要流程的数据走向和系统数据的依赖关系没有问题。 然后以一个数据实体不能承载太多职能为原则，将承担太多职能的部分进行拆分。 然后再分析系统的核心是否和当前系统实现的一致，如果不一致需要对模块进行拆分。 然后根据角色和流程梳理后的结果，按角色和其所需要的动作整理出新的流程图。 总体来讲，主要是根据流程、角色和关键元素做为切入点，然后将不同流程划分出不同阶段来归类分析，根据不同阶段拆分出不同的模块，然后再和产品和研发论证可行性。
上面主要讲了大体上应该怎么拆，下面主要讲细节上该怎么入手。
一般来说，系统功能从表拆分比较容易，因为业务流程基本都会围绕实体表运转，并关联多个实体进行交互。主要的拆分依据是：
数据实体职能只做最核心的一件事 业务流程归类按涉及实体进行归类，看能否分为多个阶段 由数据依赖交叉的频率决定是否把模块拆分，如果交互频繁可以合并 然后就是抽象的处理，越是底层的服务，越要注意抽象，主要为了减少变更，避免影响其他服务，因为很难确认底层的变更对上游系统的影响范围。
几个常见的抽象思路：
被动抽象法 如果两个或多个服务使用同一个业务逻辑，就把这个业务逻辑抽象成公共服务。适合代码量不大、维护人员很少、处于探索阶段的系统。
同层级之间的模块是禁止相互调用的。如果调用了，就需要将两个服务抽象成公共服务，让上层对两个服务进行聚合，目的是为了让系统结构从上到下是一个倒置的树形， 保证不会出现引用交叉循环的情况
动态辅助表 适用于规模稍微大一点的团队或系统。
如果一个表被多个开发组使用，而不同的业务特性也不同，可以在主表内存储 type，然后再根据 type 去对应不同的辅助表。例如：普通商品保存在表 order 和表 order_product_extra 中，定制类商品保存在 order_customize_extra 中。
强制标准接口 底层服务只做标准的服务，业务的个性部分都由业务自己完成。算是比较常见的做法。
总结 先从上到下做业务流程梳理，将流程归类聚合；然后从不同的领域聚合中找出交互所需主要实体，根据流程中主要实体之间的数据依赖程度决定是否拆分（从下到上看）； 把不同的实体和动作拆分成多个模块后，再根据业务流程归类，划分出最终的模块（最终汇总）。
极客时间的课，总结往往特别精华……</description>
    </item>
    
    <item>
      <title>Raft一致性算法论文中文翻译</title>
      <link>https://dabao-zhao.github.io/posts/raft%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/</link>
      <pubDate>Thu, 10 Aug 2023 16:36:39 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/raft%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/</guid>
      <description>对 https://github.com/maemual/raft-zh_cn 的一个副本，防止丢失
寻找一种易于理解的一致性算法（扩展版） 寻找一种易于理解的一致性算法（扩展版） 摘要 1 介绍 2 复制状态机 3 Paxos 算法的问题 4 为了可理解性的设计 5 Raft 一致性算法 5.1 Raft 基础 5.2 领导人选举 5.3 日志复制 5.4 安全性 5.4.1 选举限制 5.4.2 提交之前任期内的日志条目 5.4.3 安全性论证 5.5 跟随者和候选人崩溃 5.6 时间和可用性 6 集群成员变化 7 日志压缩 8 客户端交互 9 算法实现和评估 9.1 可理解性 9.2 正确性 9.3 性能 10 相关工作 11 结论 12 感谢 参考 摘要 Raft 是一种为了管理复制日志的一致性算法。它提供了和 Paxos 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如领导人选举、日志复制和安全性。同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。一项用户研究的结果表明，对于学生而言，Raft 算法比 Paxos 算法更加容易学习。Raft 算法还包括一个新的机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。</description>
    </item>
    
    <item>
      <title>共识Raft：如何保证多机房数据的一致性</title>
      <link>https://dabao-zhao.github.io/posts/%E5%85%B1%E8%AF%86raft%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%A4%9A%E6%9C%BA%E6%88%BF%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/</link>
      <pubDate>Wed, 09 Aug 2023 11:20:37 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%85%B1%E8%AF%86raft%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%A4%9A%E6%9C%BA%E6%88%BF%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/</guid>
      <description>答：基于 Raft 协议的分布式数据服务
参考资料 raft 算法的动画展示 官网 中文论文 </description>
    </item>
    
    <item>
      <title>同城双活：如何实现机房之间的数据同步</title>
      <link>https://dabao-zhao.github.io/posts/%E5%90%8C%E5%9F%8E%E5%8F%8C%E6%B4%BB%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%9C%BA%E6%88%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/</link>
      <pubDate>Wed, 09 Aug 2023 10:58:10 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%90%8C%E5%9F%8E%E5%8F%8C%E6%B4%BB%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%9C%BA%E6%88%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/</guid>
      <description>答：otter
TODO 后面再找一个 otter 的教程吧
常见的网络延迟参考 指一次 RTT 请求
同机房服务器：0.1 ms 同城服务器（100 公里以内）：1ms 北京到上海：38ms 北京到广州：53ms </description>
    </item>
    
    <item>
      <title>Token：如何降低用户身份鉴权的流量压力？</title>
      <link>https://dabao-zhao.github.io/posts/token%E5%A6%82%E4%BD%95%E9%99%8D%E4%BD%8E%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E9%89%B4%E6%9D%83%E7%9A%84%E6%B5%81%E9%87%8F%E5%8E%8B%E5%8A%9B/</link>
      <pubDate>Tue, 08 Aug 2023 17:21:24 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/token%E5%A6%82%E4%BD%95%E9%99%8D%E4%BD%8E%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E9%89%B4%E6%9D%83%E7%9A%84%E6%B5%81%E9%87%8F%E5%8E%8B%E5%8A%9B/</guid>
      <description>答：用 JWT
TODO 后面再找一个 JWT 的教程吧</description>
    </item>
    
    <item>
      <title>缓存一致：读多写少时，如何解决数据更新缓存不同步</title>
      <link>https://dabao-zhao.github.io/posts/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E8%AF%BB%E5%A4%9A%E5%86%99%E5%B0%91%E6%97%B6%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0%E7%BC%93%E5%AD%98%E4%B8%8D%E5%90%8C%E6%AD%A5/</link>
      <pubDate>Tue, 08 Aug 2023 15:22:18 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E8%AF%BB%E5%A4%9A%E5%86%99%E5%B0%91%E6%97%B6%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0%E7%BC%93%E5%AD%98%E4%B8%8D%E5%90%8C%E6%AD%A5/</guid>
      <description>主要讲临时缓存和长期缓存，还有如何保障缓存数据的一致性
临时缓存和长期缓存 这里的临时缓存是指 TTL 比较短的缓存，长期缓存指 TTL 比较长的缓存而不是常驻的缓存
缓存更新办法 一般情况下简单数据可以在更新的时候清理缓存，等下次读取时刷新缓存，防止并发修改导致临时数据进入缓存。
复杂数据的更新可以使用 Maxwell 和 Canal 对 MySQL 的更新进行监控，这样变更信息会推送到 Kafka，然后脚本就可以进行消费去更新缓存
还介绍了版本号更新缓存和脚本遍历更新缓存，个人认为用处不大
缓存穿透问题 文里这块长期缓存和缓存穿透放一块了，其实容易让人认为长期缓存解决了缓存穿透的问题，实际上两者的关系不大。
百度百科对缓存穿透的解释是：缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，进而给数据库带来压力。
临时缓存 + 长期热缓存的实现 // 尝试从缓存中直接获取用户信息 userinfo, err := Redis.Get(&amp;#34;user_info_9527&amp;#34;) if err != nil { return nil, err } //缓存命中找到，直接返回用户信息 if userinfo != nil { return userinfo, nil } //set 检测当前是否是热数据 //之所以没有使用Bloom Filter是因为有概率碰撞不准 //如果key数量超过千个，建议还是用Bloom Filter //这个判断也可以放在业务逻辑代码中，用配置同步做 isHotKey, err := Redis.SISMEMBER(&amp;#34;hot_key&amp;#34;, &amp;#34;user_info_9527&amp;#34;) if err != nil { return nil, err } //如果是热key if isHotKey { //没有找到就认为数据不存在 //可能是被删除了 return &amp;#34;&amp;#34;, nil } //没有命中缓存，并且没被标注是热点，被认为是临时缓存，那么从数据库中获取 //设置更新锁set user_info_9527_lock nx ex 5 //防止多个线程同时并发查询数据库导致数据库压力过大 lock, err := Redis.</description>
    </item>
    
    <item>
      <title>结构梳理：大并发下，你的数据库表可能成为性能隐患</title>
      <link>https://dabao-zhao.github.io/posts/%E7%BB%93%E6%9E%84%E6%A2%B3%E7%90%86%E5%A4%A7%E5%B9%B6%E5%8F%91%E4%B8%8B%E4%BD%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E5%8F%AF%E8%83%BD%E6%88%90%E4%B8%BA%E6%80%A7%E8%83%BD%E9%9A%90%E6%82%A3/</link>
      <pubDate>Tue, 08 Aug 2023 14:12:36 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%BB%93%E6%9E%84%E6%A2%B3%E7%90%86%E5%A4%A7%E5%B9%B6%E5%8F%91%E4%B8%8B%E4%BD%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E5%8F%AF%E8%83%BD%E6%88%90%E4%B8%BA%E6%80%A7%E8%83%BD%E9%9A%90%E6%82%A3/</guid>
      <description>主要以用户中心为例，讲解如何对读多写少的系统进行高并发优化。
将数据分为了四种类型：数据实体表，辅助查询表，实体关系表，历史数据表。然后根据不同类型讲述了不同的缓存策略。
数据实体表 每行数据代表一个实体，会存在一个唯一 ID。 需要根据业务需要对表进行精简，只保留所需字段，其他字段可以纵向拆分到辅助查询表。 数据小了之后，能减少 B+Tree 的层次，查询和传输也会更快。 对于这个表来说，一般会使用 前缀_ID 作为 key 进行缓存。对于一些组合条件或者对数据会做计算的一般会采用定期更新缓存的办法或者单独分出一个从库。
辅助查询表 目的是拆分出使用频率不高的字段。这里主要是提醒要定期对数据进行整理核对来保障数据的同步和完整。
实体关系表 三种关系：1:n、n:1、m:n
尽量减少 m:n 出现的可能性，如果出现要额外用一个关系表来维护关联关系。关系表在高并发系统中一般会降低一致性 要求来满足高并发的情况。主要可能会出现多级依赖。
历史数据表 一般不会去做缓存，数据量大而且增长量也大
判断是否适合缓存的核心思路 能够通过 ID 快速匹配的实体，以及通过关系快速查询的数据，适合放在长期缓存当中 通过组合条件筛选统计的数据，也可以放到临时缓存，但是更新有延迟 数据增长量大或者跟设计初衷不一样的表数据，这种不适合、也不建议去做做缓存 总结 核心就是要对数据进行归类，然后再根据归类做对应的处理</description>
    </item>
    
    <item>
      <title>Linux 下安装和升级 Go</title>
      <link>https://dabao-zhao.github.io/posts/linux%E4%B8%8B%E5%AE%89%E8%A3%85%E5%92%8C%E5%8D%87%E7%BA%A7go/</link>
      <pubDate>Tue, 08 Aug 2023 09:56:13 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/linux%E4%B8%8B%E5%AE%89%E8%A3%85%E5%92%8C%E5%8D%87%E7%BA%A7go/</guid>
      <description>安装 wget https://golang.google.cn/dl/go1.20.7.linux-amd64.tar.gz rm -rf /usr/local/go tar -C /usr/local -xzf go1.20.7.linux-amd64.tar.gz 打开 ~/.bashrc 并输入
export GOROOT=/usr/local/go export GOPATH=$HOME/gowork export GOBIN=$GOPATH/bin export PATH=$GOPATH:$GOBIN:$GOROOT/bin:$PATH 保存并关闭文件，输入以下命令以使更改生效
source ~/.bashrc 升级 操作与安装一致</description>
    </item>
    
  </channel>
</rss>
