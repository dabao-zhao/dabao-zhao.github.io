<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>赵大宝的博客</title>
    <link>https://dabao-zhao.github.io/</link>
    <description>Recent content on 赵大宝的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 21 Aug 2023 14:12:05 +0800</lastBuildDate><atom:link href="https://dabao-zhao.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>网关编程：如何通过用户网关和缓存降低研发成本</title>
      <link>https://dabao-zhao.github.io/posts/%E7%BD%91%E5%85%B3%E7%BC%96%E7%A8%8B%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E7%94%A8%E6%88%B7%E7%BD%91%E5%85%B3%E5%92%8C%E7%BC%93%E5%AD%98%E9%99%8D%E4%BD%8E%E7%A0%94%E5%8F%91%E6%88%90%E6%9C%AC/</link>
      <pubDate>Mon, 21 Aug 2023 14:12:05 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%BD%91%E5%85%B3%E7%BC%96%E7%A8%8B%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E7%94%A8%E6%88%B7%E7%BD%91%E5%85%B3%E5%92%8C%E7%BC%93%E5%AD%98%E9%99%8D%E4%BD%8E%E7%A0%94%E5%8F%91%E6%88%90%E6%9C%AC/</guid>
      <description>外网网关功能 蜘蛛嗅探识别 对于网络资源的非法引用可以检测请求 refer，如果 refer 不是本站域名就拒绝用户请求。
对于机器人抓取首先分为两类：匿名用户和登录用户。对于匿名用户需要根据 IP 记录统计请求排行时间块，分析请求热点 IP，请求频率过高的 IP 会被重点关注； 对于登录用户用时间块统计记录单个用户的请求次数及频率，超过一定程度就拒绝请求，同时重点关注该用户。
对于重点关注的用户请求时，可以通过网关对特定用户或 IP 动态注入 JS 嗅探代码，这个代码会在 Cookie 及 LocalStorage 内写入特殊密文。前端 JS 代码 检测到密文后，就会进入反机器人模式。反机器人模式可以识别客户端是否有鼠标移动及点击动作，以此判断用户是否为机器人。确认用户没问题以后，才会对服务端发 送再次签名的密文请求解锁。如果客户端一直没有回馈，就自动将怀疑用户列为准备封禁的用户，并封禁该请求，当一个IP被封禁的请求达到一定程度就会进行封禁。
网关鉴权与用户中心解耦 可以通过外网网关请求用户中心进行身份鉴定，如果鉴定通过，用户的信息就会通过 header 传递给后面的服务，业务的 API 只需接收 header 中附带的用户信息即可直接工作。
内网网关服务 失败重试 服务返回 500、403 或 504 错误时，网关不会马上返回错误，而是让请求等待一会儿后，再次重试，或者直接返回上次的缓存内容
平滑重启 先让服务停止接收新的请求，等待之前的请求处理完成，如果等待超过10秒则直接退出。
内外网关综合应用 服务接口缓存 利用网关实现一些接口返回内容的缓存，适合用在服务降级场景，用它短暂地缓解用户流量的冲击，或者用于降低内网流量的冲击。
这种缓存的数据大小建议不超过 5KB
服务监控 周期性地聚合访问日志中的错误，将其汇总起来，通过聚合汇总不同接口的请求的错误个数，格式类似“30 秒内出现 500 错误 20 个，504 报错 15 个，某域名接口响应速度大于 1 秒的情况有 40 次”来分析服务状态。</description>
    </item>
    
    <item>
      <title>存储成本：如何推算日志中心的实现成本</title>
      <link>https://dabao-zhao.github.io/posts/%E5%AD%98%E5%82%A8%E6%88%90%E6%9C%AC%E5%A6%82%E4%BD%95%E6%8E%A8%E7%AE%97%E6%97%A5%E5%BF%97%E4%B8%AD%E5%BF%83%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%88%90%E6%9C%AC/</link>
      <pubDate>Mon, 21 Aug 2023 11:28:39 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%AD%98%E5%82%A8%E6%88%90%E6%9C%AC%E5%A6%82%E4%BD%95%E6%8E%A8%E7%AE%97%E6%97%A5%E5%BF%97%E4%B8%AD%E5%BF%83%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%88%90%E6%9C%AC/</guid>
      <description>不同类型的系统，我们的投入侧重点：
读多写少的服务要重点“堆“内存和网络； 强一致服务更关注系统隔离和拆分； 写多读少的系统更加注重存储性能优化； 读多写多的系统更加关注系统的调度和系统类型的转变。 节省成本 临时应对流量方案 减少我们保存日志的周期，从保存 30 天改为保存 7 天，可以节省四分之三的空间； 非核心业务和核心业务的日志区分开，非核心业务只存 7 天，核心业务则存 30 天； 减少日志量，这需要投入人力做分析。可以适当缩减稳定业务的排查日志的输出量； 如果服务器多或磁盘少，服务器 CPU压力不大，数据可以做压缩处理，可以节省一半磁盘； 高并发写的存储冷热分离 当写需求激增时，大量的写用 SSD扛，冷数据存储用普通硬盘
网卡 千兆的网卡实际测试传输速度大概是 100MB/s 左右
万兆的网卡实际测试传输速度大概是 900MB/s 左右</description>
    </item>
    
    <item>
      <title>业务缓存：元数据服务如何实现</title>
      <link>https://dabao-zhao.github.io/posts/%E4%B8%9A%E5%8A%A1%E7%BC%93%E5%AD%98%E5%85%83%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Mon, 21 Aug 2023 09:45:49 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E4%B8%9A%E5%8A%A1%E7%BC%93%E5%AD%98%E5%85%83%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0/</guid>
      <description>感觉没有讲清楚，具体什么是对象存储，和分布式存储的区别是什么？
分布式文件存储服务 功能 动态扩容 负载均衡 文件分片或合并 文件归档 冷热点文件管理加速 问题 磁盘监控：监控磁盘的寿命、容量、inode 剩余，同时我们还要故障监控警告及日常维护； 文件管理：使用分布式存储控制器对文件做定期、冷热转换、定期清理以及文件归档等工作。 确保服务稳定：我们还要关注分布式存储副本同步状态及服务带宽。如果服务流量过大，运维和研发还需要处理好热点访问文件缓存的问题。 业务定制化：一些稍微个性点的需求，比如在文件中附加业务类型的标签、增加自动 TTL 清理，现有的分布式存储服务可能无法直接支持，还需要我们阅读相关源码，进一步改进代码才能实现功能。 硬件更新：服务器用的硬盘寿命普遍不长，特别是用于存储服务的硬盘，需要定期更换（比如三年一换） 对象存储 对象存储并不是真正按照我们指定的路径做存储的，实际上文件的路径只是一个 key。
文件块 用文件块方式压缩存储多个文件，方便管理。小文件更新时会新建一个文件来更新内容，定期整理数据时再把新老数据合并写到新的块里，然后清理掉老数据
大文本 流量会先到 CDN，CDN 找不到待查文件时，会回源到对象存储查找，如果对象存储也找不到，则回源到服务端生成。
文件的云中转 文件传输服务给文件发送方生成一个临时授权 token，再将这个文件上传到对象存储，上传成功后，接收方通过地址即可获取到授权 token，进行多线程下载，而临时文件过期后就会自动清除。
不同存储技术的对比 表头应该依次是硬盘管理、分布式存储、对象存储</description>
    </item>
    
    <item>
      <title>数据引擎：统一缓存数据平台</title>
      <link>https://dabao-zhao.github.io/posts/%E6%95%B0%E6%8D%AE%E5%BC%95%E6%93%8E%E7%BB%9F%E4%B8%80%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/</link>
      <pubDate>Fri, 18 Aug 2023 15:32:00 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E6%95%B0%E6%8D%AE%E5%BC%95%E6%93%8E%E7%BB%9F%E4%B8%80%E7%BC%93%E5%AD%98%E6%95%B0%E6%8D%AE%E5%B9%B3%E5%8F%B0/</guid>
      <description>实体数据主动缓存 建议查询服务使用类似 Redis 的纯文本长链接协议，同时还可以支持批量获取功能。
L1 缓存及热点缓存延期 多数据引擎平台 多数据引擎 lua 脚本引擎是数据推送的“发动机”，能帮我们把数据动态同步到多个数据源； Elasticsearch 负责提供全文检索功能； Pika 负责提供大容量 KV 查询功能； ClickHouse 负责提供实时查询数据的汇总统计功能； MySQL 引擎负责支撑新维度的数据查询。 </description>
    </item>
    
    <item>
      <title>流量调度：DNS、全站加速及机房负载均衡</title>
      <link>https://dabao-zhao.github.io/posts/%E6%B5%81%E9%87%8F%E8%B0%83%E5%BA%A6dns%E5%85%A8%E7%AB%99%E5%8A%A0%E9%80%9F%E5%8F%8A%E6%9C%BA%E6%88%BF%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Fri, 18 Aug 2023 11:19:35 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E6%B5%81%E9%87%8F%E8%B0%83%E5%BA%A6dns%E5%85%A8%E7%AB%99%E5%8A%A0%E9%80%9F%E5%8F%8A%E6%9C%BA%E6%88%BF%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description>DNS 域名解析及缓存 域名解析过程可以分成下面三个步骤：
客户端会请求 ISP 商提供的 DNS 解析服务，而 ISP 商的 DNS 服务会先请求根 DNS 服务器； 通过根 DNS 服务器找到.org顶级域名 DNS 服务器； 再通过顶级域名服务器找到域名主域名服务器（权威 DNS） 当请求主域名解析服务时，主域名服务器会返回服务器所在机房的入口 IP 以及建议缓存的 TTL 时间，在主域名服务返回结果给 ISP DNS 服务时，ISP 的 DNS 服务会先将这个解析结果按 TTL 规定的时间缓存到服务本地，然后才会将解析结果返回给客户端。在 ISP DNS 缓存 TTL 有效期内，同样的域名解析请求都会从 ISP 缓存直接返回结果。
// 全网刷新域名解析缓存时间 客户端本地解析缓存时间30分钟 + 市级 ISP DNS缓存时间 30分钟 + 省级 ISP DNS缓存时间 30分钟 + 主域名服务商 刷新解析服务器配置耗时 3分钟 + ... 后续ISP子网情况 略 = 域名解析实际更新时间 93分钟以上 根据经验，正常域名修改后全国 DNS 缓存需要 48 小时，才能大部分更新完毕，而刷全世界缓存需要 72 小时。</description>
    </item>
    
    <item>
      <title>流量拆分：如何通过架构设计缓解流量压力</title>
      <link>https://dabao-zhao.github.io/posts/%E6%B5%81%E9%87%8F%E6%8B%86%E5%88%86%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E7%BC%93%E8%A7%A3%E6%B5%81%E9%87%8F%E5%8E%8B%E5%8A%9B/</link>
      <pubDate>Fri, 18 Aug 2023 09:54:16 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E6%B5%81%E9%87%8F%E6%8B%86%E5%88%86%E5%A6%82%E4%BD%95%E9%80%9A%E8%BF%87%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E7%BC%93%E8%A7%A3%E6%B5%81%E9%87%8F%E5%8E%8B%E5%8A%9B/</guid>
      <description>可预估用户量的服务 可以通过压测测出单台服务器的服务在线人数，以此精确地预估带宽和服务器资源，算出一个集群（集群里包括若干服务器）需要多少资源、可以承担多少人在线进行互动，再通过调 度服务分配资源，将新来的用户分配到空闲的服务集群。
不可预估用户量的服务 聊天 主打一个信息合并，在点赞或大量用户输入同样内容的刷屏情境下，我们可以通过大数据实时计算分析用户的输入，并压缩整理大量重复的内容，过滤掉一些无用信息。 答题 可以将数据静态化，并通过 CDN 阻挡这个流量，同时为了避免出现瞬时的高峰，推荐客户端拉取时加入随机延迟几秒，再发送请求。如果请求失败后，可以通过退火算法来设置重试时间。
还可以将题目提前推送到客户端，减少拉取压力；同时为了让用户接收题目时间相对一致，可以在发送动作生效后 5 秒再弹出题目。
非抢答类型的题目，用户回答完题目后，可以先在客户端本地先做预判卷，把正确答案和解析展示给用户，然后在直播期间异步缓慢地提交用户答题结果到服务端。
点赞 客户端 客户端无需实时提交用户的所有交互，可以合并这些操作为一条消息再推送到服务端。
服务端 通过第一层写缓存本地的实时汇总来缓解大量用户的请求，将更新数据周期性地汇总后，提交到二级写缓存。之后，二级汇总所在分片的所有上层服务数值后，最终汇 总同步给核心缓存服务。接着，通过核心缓存把最终结果汇总累加起来。最后通过主从复制到多个子查询节点服务，供用户查询汇总结果。
适合一致性要求不高的计数器。
打赏 &amp;amp; 购物 可以按用户 id 做 hash 拆分，通过网关将不同用户 uid 取模后，根据范围分配到不同分片服务上，然后分片内的服务对类似的请求进行内存实时计算更新。
但 hash 分配容易出现个别热点，可以使用一致性 hash 算法，支持对局部的区域进行扩容，但也会因为算法不通用，无法人为控制，使用起来很麻烦，需要开发配套工具。
树形热迁移切片法 比如我们将全量数据拆分成 256 份，一份代表一个桶，16 个服务器每个分 16 个桶，当我们个别服务器压力过大的时候，可以给这个服务器增加两个订阅服务器去做主从同步， 迁移这个服务器的 16 个桶的数据。待同步迁移成功后，将这个服务器的请求流量拆分转发到两个 8 桶服务器，分别请求这两个订阅服务器继续对外服务，原服务器摘除回收即可。
服务切换成功后，由于是全量迁移，这两个服务同时同步了不属于自己的 8 个桶数据，这时新服务器遍历自己存储的数据，删除掉不属于自己的数据即可。当然也可以在同步 16 桶服务的 数据时，过滤掉这些数据，这个方法适用于 Redis、MySQL 等所有有状态分片数据服务。
难点在于请求的客户端不直接请求分片，而是通过代理服务去请求数据服务，只有通过代理服务才能够动态更新调度流量，实现平滑无损地转发流量。
如何让客户端知道请求哪个分片才能找到数据呢？
客户端通过算法找到分片，比如：用户 hash(uid) % 100 = 桶 id，在配置中通过桶 id 找到对应分片。 数据服务端收到请求后，将请求转发到有数据的分片。比如客户端请求 A 分片，再根据数据算法对应的分片配置找到数据在 B 分片，这时 A 分片会转发这个请求到 B，待 B 处理后返回给客户端数据。 基础建设 分布式服务：分布式队列、分布式实时计算、分布式存储。</description>
    </item>
    
    <item>
      <title>业务脚本：为什么说可编程订阅式缓存服务更有用</title>
      <link>https://dabao-zhao.github.io/posts/%E4%B8%9A%E5%8A%A1%E8%84%9A%E6%9C%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%8F%AF%E7%BC%96%E7%A8%8B%E8%AE%A2%E9%98%85%E5%BC%8F%E7%BC%93%E5%AD%98%E6%9C%8D%E5%8A%A1%E6%9B%B4%E6%9C%89%E7%94%A8/</link>
      <pubDate>Thu, 17 Aug 2023 15:58:11 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E4%B8%9A%E5%8A%A1%E8%84%9A%E6%9C%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E8%AF%B4%E5%8F%AF%E7%BC%96%E7%A8%8B%E8%AE%A2%E9%98%85%E5%BC%8F%E7%BC%93%E5%AD%98%E6%9C%8D%E5%8A%A1%E6%9B%B4%E6%9C%89%E7%94%A8/</guid>
      <description>可编程订阅式缓存服务就是对缓存的数据进行加工，例如筛选过滤、统计计算、查询、分片、数据拼合。
lua 是一个小巧的嵌入式脚本语言，通过gopher-lua 可以实现 Go 和 Lua 的交互。 在实际使用时，lua 会在内存中运行很多实例，因此最好用一个脚本管理系统来管理所有 lua 的实运行例子，以此实现脚本的统一更新、编译缓存、资源调度和控制单例。
Go 和 Lua 进行变量交互时，最好将数据在 Go 中封装，然后供 Lua 调用，这样更规范更好管理。
缓存预热与数据来源 常见的数据来源是大数据挖掘周期生成的全量数据离线文件，通过 NFS 或 HDFS 挂载定期刷新、加载最新的文件。这个方式适合数据量大且更新缓慢的数据，缺点则是加载时需要整理数据，如果情况足够复杂，800M 大小的数据要花 1～10 分钟方能加载完毕。
也可以在程序启动后扫数据表恢复数据，但这么做数据库要承受压力，建议使用专用的从库。但相对磁盘离线文件的方式，这种方式加载速度更慢。
还可以将 RocksDB 嵌入到进程中，这样做可以大幅度提高我们的数据存储容量，实现内存磁盘高性能读取和写入。RocksDB 的数据可以通过大数据生成 RocksDB 格式的数据库文件，拷贝给我们的服务直接加载。
对于离线文件加载，最好做一个 CheckSum 一类的文件，用来在加载文件之前检查文件的完整性。
数据同步 一般会通过订阅 binlog 将变更信息同步到 Kafka，再通过 Kafka 的分组消费来通知分布在不同集群中的缓存。 还可以用周期任务刷新数据的统计，通过周期任务结合 lua 自定义逻辑脚本，就能实现定期统计。这里引入了时间轮的概念。</description>
    </item>
    
    <item>
      <title>本地缓存：用本地缓存做服务会遇到哪些坑</title>
      <link>https://dabao-zhao.github.io/posts/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98%E7%94%A8%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98%E5%81%9A%E6%9C%8D%E5%8A%A1%E4%BC%9A%E9%81%87%E5%88%B0%E5%93%AA%E4%BA%9B%E5%9D%91/</link>
      <pubDate>Thu, 17 Aug 2023 14:57:34 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98%E7%94%A8%E6%9C%AC%E5%9C%B0%E7%BC%93%E5%AD%98%E5%81%9A%E6%9C%8D%E5%8A%A1%E4%BC%9A%E9%81%87%E5%88%B0%E5%93%AA%E4%BA%9B%E5%9D%91/</guid>
      <description>缺页中断 Linux 采用虚拟内存的方式管理内存，每个进程都有自己的虚拟内存空间。
在 Linux 申请的内存不会立刻从物理内存划分出来。系统数据修改时，才会发现物理内存没有分配，此时 CPU 会产生缺页中断，操作系统才会以 page 为单位把物理内存分配给程序。
系统这么设计，主要是为了降低系统的内存碎片，并且减少内存的浪费。但 page 很小，一般是 4KB，分配大内存时会频繁触发缺页中断，导致程序缓慢、不稳定的问题。
一般会在服务启动时预先申请一大块内存并填 0，然后再使用。
Swap Out Linux 会根据统计将我们长时间不访问的数据从内存里挪走，留出空间给其他活跃的内存使用，这个操作叫 Swap Out。
需要控制缓存在系统中的占用量，需要通过 LRU 淘汰掉一些不频繁访问的数据，同时建议对内存做定期扫描续热，以此预防流量突增时触发大量缺页中断导致服务卡顿、最终宕机的情况。
锁 公共数据需要加锁，如果不做互斥锁，就会出现多线程修改不一致的问题。但锁多多少少都会影响服务性能。
一般情况下就是要减少锁的粒度，来减少锁的冲突，例如：可以将一个放大量数据的经常修改的 map 拆分成 256 份甚至更多的分片，每个分片会有一个互斥锁。
还可以通过全量替换的方式实现数据更新，具体的做法是用两个指针分别指向两个 map，一个 map 用于对外服务，当拿到更新数据离线包时，另一个指针指向的 map 会加载离线全量数据。加载完毕后，两个 map 指针指向互换。
当然还有一些无锁的操作：atomic、Go 的 sync.Map、sync.Pool、Java 的 volidate、MySQL InnoDB 的 MVCC
GC 每个语言的 GC 策略不一样，对数据类型的处理也不一样。
一般情况下，GC 会定期扫描 map 中的对象来判断是否能够回收，这样的话，map 中的对象越多，GC 的速度就越慢。而 Go 的 GC 对此做了优化，如果 map 中没有指针，GC 就不会去遍历。
如果我们的数据量少，且特点是读多写多，那么将它的 struct 放到 map 中对外服务更合理； 如果我们的数据量大，且特点是读多写少，那么把数据放到一个连续内存中，通过 offset 和 length 访问会更合适。</description>
    </item>
    
    <item>
      <title>实践方案：如何用C&#43;&#43;自实现链路跟踪</title>
      <link>https://dabao-zhao.github.io/posts/%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%A1%88%E5%A6%82%E4%BD%95%E7%94%A8c&#43;&#43;%E8%87%AA%E5%AE%9E%E7%8E%B0%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA/</link>
      <pubDate>Thu, 17 Aug 2023 10:42:35 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%AE%9E%E8%B7%B5%E6%96%B9%E6%A1%88%E5%A6%82%E4%BD%95%E7%94%A8c&#43;&#43;%E8%87%AA%E5%AE%9E%E7%8E%B0%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA/</guid>
      <description>像是对前几章的实战总结
写多读少系统的主要优化思路：用分布式队列汇总日志、利用内存缓存新写入的数据、顺序写入磁盘、多服务器分片、分布式查询可拆分索引。
抓取、采集与传输 一般来说，我们需要在接口被请求时，接收传递过来的 TraceID 以及 RPCID，如果没有传递过来的 TraceID，那么自己可以用 UUID 生成一个，用于标识后续在请求期间所有的日志。
服务被请求时，建议记录一条被调用的访问日志，具体可以记录当前被请求的参数以及接口最后返回的结果、httpcode、耗时。通过这个日志，后续可以方便我们分析服务的性能和故障。
对于被请求期间的业务所产生的业务日志、错误日志，以及请求其他资源的日志，都需要做详细记录，比如 SQL 查询记录、API 请求记录以及这些请求的参数、返回、耗时。
一般有两种传输方式：
一个是通过 memcache 的长链接协议，将日志推送到我们日志收集服务上，这种推送日志请求超时超过 200ms 就会丢弃，这样能避免拖慢接口的性能。 一个是落地到本地磁盘，通过 Filebeat 实时抓取推送，将日志收集汇总起来。 以上两种方式最后都会将日志推送大 kafka，主要在于 kafka 可以简单的实现负载均衡和动态扩容。当流量超过其承受能力时，可以随时通过给服务器群组增加服务器来扩容，从而提供更好的吞吐量。
而且基于 Kafka 的分组特性，可以将服务做成两组消费服务，一组用于数据的统计，一组用于存储，通过这个方式隔离存储和实时统计服务。
存储 查询和计算 这块没懂，每个接口返回带上 trace id 的作用是？总感觉缺一点什么东西
事实上很简单，我们的 Trace SDK 会让每个接口返回响应内容的同时，在 header 中包含了 TraceID，debug 的时候使用返回 traceId 进行查询时，界面会对所有存储节点发送查询请求，通过 TraceID 从 RocksDB 拿出所有按回车分割的日志后，汇总排序即可。
服务器状态统计计算，还是利用 Kafka 的分组消费，另外启动一组服务消费日志内容，在内存中对日志进行汇总计算。</description>
    </item>
    
    <item>
      <title>跳数索引：后起新秀ClickHouse</title>
      <link>https://dabao-zhao.github.io/posts/%E8%B7%B3%E6%95%B0%E7%B4%A2%E5%BC%95%E5%90%8E%E8%B5%B7%E6%96%B0%E7%A7%80clickhouse/</link>
      <pubDate>Wed, 16 Aug 2023 15:31:25 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E8%B7%B3%E6%95%B0%E7%B4%A2%E5%BC%95%E5%90%8E%E8%B5%B7%E6%96%B0%E7%A7%80clickhouse/</guid>
      <description>并行能力 CPU 吞吐和性能 ClickHouse 会充分利用多核，对本地大量数据做快速的计算，所以一个请求就可以用光所有系统资源，官方建议 ClickHouse 的查询 QPS 限制在 100 左右。
ClickHouse 的场景如果是对用户服务的，最好对这种查询做缓存。而且，界面在加载时要设置 30 秒以上的等待时间，因为我们的请求可能在排队等待别的查询。
推荐的优化思路是：基于排序字段做范围查询过滤后，再做聚合查询。高并发查询数据的服务和缓慢查询的服务需要隔离开。
批量写入优化 批量插入数据时，客户端会将要插入的数据先在本地缓存一段时间，直到积累足够配置的 block_size 后才会把数据批量提交到服务端，以此提高写入的性能。
除了客户端做的合并，ClickHouse 的引擎 MergeTree 也做了类似的工作。MergeTree 采用了批量写入磁盘、定期合并方式（batch write-merge）。
当我们批量输入的数据量小于 min_bytes_for_wide_part 设置时，会按 compact part 方式落盘。这种方式会将落盘的数据放到一个 data.bin 文件中， merge 时会有很好的写效率，这种方式适合于小量业务数据筛选使用。
当我们批量输入的数据量大于 min_bytes_for_wide_part 设置时，会按 wide part 方式落盘，落盘数据的时候会按字段生成不同的文件。这个方式适用于字 段较多的数据，merge 相对会慢一些，但是对于指定参与计算列的统计计算，并行吞吐写入和计算能力会更强，适合分析指定小范围的列计算。
查询效率 Clickhouse 有两种索引方式，一种是主键索引，这个是在建表时就需要指定的；另一种是跳表索引，用来跳过一些数据。
ClickHouse 的查询工作流程：
根据查询条件，查询过滤出要查询需要读取的 data part 文件夹范围； 根据 data part 内数据的主键索引、过滤出要查询的 granule； 使用跳表索引跳过不符合的 granule； 范围内数据进行计算、汇总、统计、筛选、排序； 返回结果。 只有第四步里的几个操作是并行的
很难做索引查询优化的原因：
大部分数据的特征不是很明显、建立的索引区分度不够 目录过多，有多份数据同时散落在多个 data parts 文件夹内，需要加载所有 date part 的索引挨个查询 其他特性 物化视图 物化视图会将数据源的数据通过聚合函数实时统计计算，每次我们查询这个表，就能获得表规定的统计结果</description>
    </item>
    
    <item>
      <title>实时统计：链路跟踪实时计算中的实用算法</title>
      <link>https://dabao-zhao.github.io/posts/%E5%AE%9E%E6%97%B6%E7%BB%9F%E8%AE%A1%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%94%A8%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 16 Aug 2023 14:27:45 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%AE%9E%E6%97%B6%E7%BB%9F%E8%AE%A1%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%94%A8%E7%AE%97%E6%B3%95/</guid>
      <description>URL 去参数聚合 主要是 RESTful 的接口需要做处理：
同一个 API 由于不同的参数无法归类 同一网址不同的 method 操作在 RESTful 中实际也是不同的实现 人工配置替换模版 也就是人工配置出一个 URL 规则，用来筛选出符合规则的日志并替换掉关键部分的参数。
类Radix tree效果： /user - /* - - /info - - - :GET - - - :PUT - - /friend - - - /* (代表这里替换) - - - - :DELETE 具体实现是将网址通过 / 进行分割，逐级在前缀搜索树查找
数据特征筛选 //根据数据特征过滤网址内参数 function filterUrl($url) { $urlArr = explode(&amp;#34;/&amp;#34;, $url); foreach ($urlArr as $urlIndex =&amp;gt; $urlItem) { $totalChar = 0; //有多少字母 $totalNum = 0; //有多少数值 $totalLen = strlen($urlItem); //总长度 for ($index = 0; $index &amp;lt; $totalLen; $index++) { if (is_numeric($urlItem[$index])) { $totalNum++; } else { $totalChar++; } } //过滤md5 长度32或64 内容有数字 有字符混合 直接认为是md5 if (($totalLen == 32 || $totalLen == 64) &amp;amp;&amp;amp; $totalChar &amp;gt; 0 &amp;amp;&amp;amp; $totalNum &amp;gt;0) { $urlArr[$urlIndex] = &amp;#34;*md*&amp;#34;; continue; } //字符串 data 参数是数字和英文混合 长度超过3(回避v1/v2一类版本) if ($totalLen &amp;gt; 3 &amp;amp;&amp;amp; $totalChar &amp;gt; 0 &amp;amp;&amp;amp; $totalNum &amp;gt; 0) { $urlArr[$urlIndex] = &amp;#34;*data*&amp;#34;; continue; } //全是数字在网址中认为是id一类， 直接进行替换 if ($totalChar == 0 &amp;amp;&amp;amp; $totalNum &amp;gt; 0) { $urlArr[$urlIndex] = &amp;#34;*num*&amp;#34;; continue; } } return implode(&amp;#34;/&amp;#34;, $urlArr); } 时间分块统计 每个数据统计块内聚合了以下指标：</description>
    </item>
    
    <item>
      <title>引擎分片：Elasticsearch如何实现大数据检索</title>
      <link>https://dabao-zhao.github.io/posts/%E5%BC%95%E6%93%8E%E5%88%86%E7%89%87elasticsearch%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2/</link>
      <pubDate>Wed, 16 Aug 2023 10:24:58 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%BC%95%E6%93%8E%E5%88%86%E7%89%87elasticsearch%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A3%80%E7%B4%A2/</guid>
      <description>Elasticsearch 架构 Master 节点：负责集群内调度决策，集群状态、节点信息、索引映射、分片信息、路由信息，Master 真正主节点是通过选举诞生的，一般一个集群内至少要有三个 Master 可竞选成员，防止主节点损坏； Data 存储节点：用于存储数据及计算，分片的主从副本，热点节点，冷数据节点； Client 协调节点：协调多个副本数据查询服务，聚合各个副本的返回结果，返回给客户端； Kibana 计算节点：作用是实时统计分析、聚合分析统计数据、图形聚合展示。 Elasticsearch 索引 Elasticsearch 其底层全文检索使用的是 Lucene 引擎，但 Lucene 不支持分布式，Elasticsearch 的分布式功能是通过基础分片来实现的。
索引通过倒排索引实现。 Elasticsearch 集群的索引保存在 Lucene 的 segment 文件中，segment 主要保存了三种索引：
Term Index（单词词典索引）：用于关键词（Term）快速搜索，Term index 是基础 Trie 树改进的 FST（Finite State Transducer 有限状态传感器，占用内存少） 实现的二级索引。平时这个树会放在内存中，用于减少磁盘 IO 加快 Term 查找速度，检索时会通过 FST 快速找到 Term Dictionary 对应的词典文件 block。 Term Dictionary（单词词典）：单词词典索引中保存的是单词（Term）与 Posting List 的关系，而这个单词词典数据会按 block 在磁盘中排序压缩保存，相比 B-Tree 更节省空间，其中保存了单词的前缀后缀，可以用于近似词及相似词查询，通过这个词典可以找到相关的倒排索引列表位置。 Posting List（倒排列表）：倒排列表记录了关键词 Term 出现的文档 ID，以及其所在文档中位置、偏移、词频信息，这是我们查找的最终文档列表，我们拿到这些就可以拿去排序合并了。 Elasticsearch 写入 在数据入库之前，会先进行分词，过滤掉无用符号等分隔词，找出文档中每个关键词（Term）在文档中的位置及频率权重；然后，将这些关键词保存在 Term Index 以及 Term Dictionary 内；最后，将每个关键词对应的文档 ID 和权重、位置等信息排序合并到 Posting List 中进行保存。</description>
    </item>
    
    <item>
      <title>链路追踪：如何定制一个分布式链路跟踪系统</title>
      <link>https://dabao-zhao.github.io/posts/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E5%A6%82%E4%BD%95%E5%AE%9A%E5%88%B6%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Tue, 15 Aug 2023 15:50:03 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E5%A6%82%E4%BD%95%E5%AE%9A%E5%88%B6%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F/</guid>
      <description>Metrics、Tracing 和 Logging 常见监控系统主要有三种类型：Metrics、Tracing 和 Logging。
常见的开源 Metrics 有 Zabbix、Nagios、Prometheus、InfluxDb、OpenFalcon。主要做各种量化指标汇总统计，比如监控系统的容量剩余、每秒请求量、平均响应速度、某个时段请求量多少。
常见的开源链路跟踪有 Jaeger、Zipkin、Pinpoint、Skywalking。主要是通过分析每次请求链路监控分析的系统，可以通过 TraceID 查找一次请求的依赖及调用链路，分析故障点和传导过程的耗时。
常见的开源 Logging 有 ELK、Loki、Loggly。主要是对文本日志的收集归类整理，可以对错误日志进行汇总、警告，并分析系统错误异常等情况。
三种监控体系最大的区别就是日志格式标准。
OpenTelemetry 标准 主要是对数据采集和标准规范的统一，其他的不参与。
TODO 后面出个介绍
分布式链路跟踪系统 主要功能服务：
监控日志标准 埋点 SDK（AOP 或侵入式） 日志收集 分布式日志传输 分布式日志存储 分布式检索计算 分布式实时分析 个性化定制指标盘 系统警告 ELK 即可实现以上功能：
日志收集（Filebeat） 日志传输（Kafka+Logstash） 日志存储（Elasticsearch） 检索计算（Elasticsearch + Kibana） 实时分析（Kibana） 个性定制表格查询（Kibana） 这样之后只需要制定日志格式和埋点 SDK 即可实现一个具有分布式链路跟踪、 Metrics、日志分析的系统。 TODO 后面出个介绍
原理 在请求发起方发送请求时或服务被请求时生成一个 UUID（TraceID），被请求期间的业务产生的任何日志（Warning、Info、Debug、Error）、任何依赖资源 请求（MySQL、Kafka、Redis）、任何内部接口调用（Restful、Http、RPC）都会带上这个 UUID。 这样，当我们把所有拥有同样 UUID 的日志收集起来时，就可以根据时间（有误差）、RPCID 或 SpanID，将它们按依赖请求顺序串起来。 只要日志足够详细，我们就能监控到系统大部分的工作状态，比如用户请求一个服务会调用多少个接口，每个数据查询的 SQL 以及具体耗时调用的内网请求参数是什么、 调用的内网请求返回是什么、内网被请求的接口又做了哪些操作、产生了哪些异常信息等等。 同时，我们可以通过对这些日志做归类分析，分析项目之间的调用关系、项目整体健康程度、 对链路深挖自动识别出故障点等，帮助我们主动、快速地查找问题。
TraceID、RPCID、SpanID TraceID 用于标识某一次具体的请求 ID。 SpanID 用于标识单个服务中的一个操作。 RPCID 用于标识操作在整个调用链路中的位置。</description>
    </item>
    
    <item>
      <title>稀疏索引：为什么高并发写不推荐关系数据库</title>
      <link>https://dabao-zhao.github.io/posts/%E7%A8%80%E7%96%8F%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E9%AB%98%E5%B9%B6%E5%8F%91%E5%86%99%E4%B8%8D%E6%8E%A8%E8%8D%90%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Tue, 15 Aug 2023 09:51:35 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%A8%80%E7%96%8F%E7%B4%A2%E5%BC%95%E4%B8%BA%E4%BB%80%E4%B9%88%E9%AB%98%E5%B9%B6%E5%8F%91%E5%86%99%E4%B8%8D%E6%8E%A8%E8%8D%90%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>B+Tree MySQL 几乎所有查询都是通过索引去缩小扫描数据的范围，然后再回到表中对范围内数据进行遍历加工、过滤，最终拿到我们的业务需要的数据。
B+Tree 的特点在于只有在最底层才会存储真正的数据 ID，通过这个 ID 就可以提取到数据的具体内容，同时 B+Tree 索引最底层的数据是按索引字段顺序进行存储的。 通过这种设计方式，我们只需进行 1～3 次 IO（树深度决定了 IO 次数）就能找到所查范围内序好的数据，而树形的索引最影响查询效率的是树的深度以及数据量。
MySQL 的索引是使用 Page 作为单位进行存储的，而每页只能存储 16KB（innodb_page_size）数据。如果我们每行数据的索引是1KB，那么除去 Page 页的一些固定 结构占用外，一页只能放 16 条数据，这导致树的一些分支装不下更多数据时，就需要对索引的深度再加一层。就可以推导出：索引第一层放 16 条，树第二层大概能放 2 万条， 树第三层大概能放 2400 万条，三层的深度 B+Tree 按主键查找数据每次查询需要 3 次 IO（一层索引在内存，IO 两次索引，最后一次是拿数据），这也是一些文章会推荐说 MySQL 的单表记录最好不要超过 2000 万条的原因。
不过不是绝对的，和每行数据的大小有关系
B+Tree 其实有些离题了……但是硬说也能原回来，因为索引太多也是会影响写入速度的
文中最后总结不适合的原因是：主从的部署模式，大批量的插入会造成主库响应缓慢、主从同步延迟增大等问题
稀疏索引 LSM Tree TODO 后面再找个教程
主要以 RocksDB 为例介绍。
RocksDB LSM 新数据写入时会在内存中暂存，当内存中数据积累到一定程度后，会将内存中数据和索引做顺序写，落地形成一个数据块。新生成的数据块会保存在 Level 0 层， 每一层的数据块和数据量超过一定程度时，RocksDB 合并不同 Level 的数据，将多个数据块内的数据和索引合并在一起，并推送到 Level 的下一层。
当我们查询一个 key 的时候，RocksDB 会先查内存。如果没找到，会从 Level 0 层到下层，每层按生成最新到最老的顺序去查询每层的数据块。同时为了减少 IO 次数，每个数据块 都会有一个 BloomFilter 辅助索引，来辅助确认这个数据块中是否可能有对应的 Key；如果当前数据块没有，那么可以快速去找下一个数据块，直到找到为止。</description>
    </item>
    
    <item>
      <title>分布式事务：多服务的2PC、TCC都是怎么实现的</title>
      <link>https://dabao-zhao.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%A4%9A%E6%9C%8D%E5%8A%A1%E7%9A%842pctcc%E9%83%BD%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/</link>
      <pubDate>Mon, 14 Aug 2023 14:51:34 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E5%A4%9A%E6%9C%8D%E5%8A%A1%E7%9A%842pctcc%E9%83%BD%E6%98%AF%E6%80%8E%E4%B9%88%E5%AE%9E%E7%8E%B0%E7%9A%84/</guid>
      <description>XA 协议 三个角色 应用（AP） 应用是具体的业务逻辑代码实现，业务逻辑通过请求事务协调器开启全局事务，在事务协调器注册多个子事务后，业务代码会依次给所有参与事务的子业务下发请求。 待所有子业务提交成功后，业务代码根据返回情况告诉事务协调器各个子事务的执行情况，由事务协调器决策子事务是提交还是回滚（有些实现是事务协调器发请求给子服务）。
事务协调器（TM） 用于创建主事务，同时协调各个子事务。事务协调器会根据各个子事务的执行情况，决策这些子事务最终是提交执行结果，还是回滚执行结果。此外，事务协调器很多时候还会自动帮我们提交事务；
资源管理器（RM） 是一种支持事务或 XA 协议的数据资源，比如 MySQL、Redis 等
两个阶段 Prepare 阶段 在 Prepare 阶段，事务协调器会通过 xid（事务唯一标识，由业务或事务协调器生成）协调多个资源管理器执行子事务，所有子事务执行成功后会向事务协调器汇报。 这时的子事务执行成功是指事务内 SQL 执行成功，并没有执行事务的最终 commit（提交），所有子事务是提交还是回滚，需要等事务协调器做最终决策。
Commit 阶段 当事务协调器收到所有资源管理器成功执行子事务的消息后，会记录事务执行成功，并对子事务做真正提交。如果 Prepare 阶段有子事务失败，或者事务协调器在一段 时间内没有收到所有子事务执行成功的消息，就会通知所有资源管理器对子事务执行回滚的操作。
事务的几个状态 ACTIVE：子事务 SQL 正在执行中； IDLE：子事务执行完毕等待切换 Prepared 状态，如果本次操作不参与回滚，就可以直接提交完成； PREPARED：子事务执行完毕，等待其他服务实例的子事务全部 Ready。 COMMITED/FAILED：所有子事务执行成功 / 失败后，一起提交或回滚。 2PC 缺点：
同步阻塞问题：事务的执行过程中，所有参与事务的节点都会对其占用的公共资源加锁，导致其他访问公共资源的进程或者线程阻塞。 单点故障问题：如果事务管理器发生故障，则资源管理器会一直阻塞。 数据不一致问题：如果在 Commit 阶段，由于网络或者部分资源管理器发生故障，导致部分资源管理器没有接收到事务管理器发送过来的 Commit 消息，会引起数据不一致的问题。 无法解决的问题：如果在 Commit 阶段，事务管理器发出 Commit 消息后宕机，并且唯一接收到这条 Commit 消息的资源管理器也宕机了，则无法确认事务是否已经提交。 3PC 3PC 模型把 2PC 模型中的 Prepare 阶段一分为二，最终形成 3 个阶段：
CanCommit 阶段：为了减少因等待锁定数据导致的超时情况，提高事务成功率，事务协调器会发送消息确认资源管理器的资源锁定情况，以及所有子事务的数据库锁定数据的情况。 PreCommit 阶段：执行 2PC 的 Prepare 阶段； DoCommit 阶段：执行 2PC 的 Commit 阶段。 缺点：</description>
    </item>
    
    <item>
      <title>系统隔离：如何应对高并发流量冲击</title>
      <link>https://dabao-zhao.github.io/posts/%E7%B3%BB%E7%BB%9F%E9%9A%94%E7%A6%BB%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E9%AB%98%E5%B9%B6%E5%8F%91%E6%B5%81%E9%87%8F%E5%86%B2%E5%87%BB/</link>
      <pubDate>Mon, 14 Aug 2023 14:07:19 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%B3%BB%E7%BB%9F%E9%9A%94%E7%A6%BB%E5%A6%82%E4%BD%95%E5%BA%94%E5%AF%B9%E9%AB%98%E5%B9%B6%E5%8F%91%E6%B5%81%E9%87%8F%E5%86%B2%E5%87%BB/</guid>
      <description>感觉又是水的一篇
首先是部署的隔离，内网和外网部署要物理隔离 然后是网关的隔离，内网网关和外网网关隔离，流量经过外网网关、外网、内网网关才能到内网。其中外网网关和内网网关都可以做限流和熔断。外网服务要保证内网 网关断开后，仍旧能正常独立运转一小时以上。 然后是减少外网和内网的 api 互动，内外通过缓存来交互数据，但要保证数据同步是单向的。如果内网想要修改数据可以通过请求外网接口，外网对内网的数据同步可以通过消息队列。 然后是讲了消息队列的优点：
队列拥有良好吞吐并且能够动态扩容，可应对各种流量冲击场景； 可通过动态控制内网消费线程数，从而实现内网流量可控； 内网消费服务在高峰期可以暂时离线，内网服务可以临时做一些停机升级操作； 内网服务如果出现 bug，导致消费数据丢失，可以对队列消息进行回放实现重新消费； Kafka 是分区消息同步，消息是顺序的，很少会乱序，可以帮我们实现顺序同步； 消息内容可以保存很久，加入 TraceID 后查找方便并且透明，利于排查各种问题。 感觉水的一部分原因是太理论了，和实战没啥关系……</description>
    </item>
    
    <item>
      <title>强一致锁：如何解决高并发下的库存争抢问题</title>
      <link>https://dabao-zhao.github.io/posts/%E5%BC%BA%E4%B8%80%E8%87%B4%E9%94%81%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E7%9A%84%E5%BA%93%E5%AD%98%E4%BA%89%E6%8A%A2%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 14 Aug 2023 11:11:17 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%BC%BA%E4%B8%80%E8%87%B4%E9%94%81%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8B%E7%9A%84%E5%BA%93%E5%AD%98%E4%BA%89%E6%8A%A2%E9%97%AE%E9%A2%98/</guid>
      <description></description>
    </item>
    
    <item>
      <title>领域拆分：如何合理地拆分系统</title>
      <link>https://dabao-zhao.github.io/posts/%E9%A2%86%E5%9F%9F%E6%8B%86%E5%88%86%E5%A6%82%E4%BD%95%E5%90%88%E7%90%86%E5%9C%B0%E6%8B%86%E5%88%86%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Thu, 10 Aug 2023 17:10:18 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E9%A2%86%E5%9F%9F%E6%8B%86%E5%88%86%E5%A6%82%E4%BD%95%E5%90%88%E7%90%86%E5%9C%B0%E6%8B%86%E5%88%86%E7%B3%BB%E7%BB%9F/</guid>
      <description>以供货协商流程为例讲述了如何拆分系统。
首先进行流程分析，要多和产品、研发团队沟通，以确定主要流程的数据走向和系统数据的依赖关系没有问题。 然后以一个数据实体不能承载太多职能为原则，将承担太多职能的部分进行拆分。 然后再分析系统的核心是否和当前系统实现的一致，如果不一致需要对模块进行拆分。 然后根据角色和流程梳理后的结果，按角色和其所需要的动作整理出新的流程图。 总体来讲，主要是根据流程、角色和关键元素做为切入点，然后将不同流程划分出不同阶段来归类分析，根据不同阶段拆分出不同的模块，然后再和产品和研发论证可行性。
上面主要讲了大体上应该怎么拆，下面主要讲细节上该怎么入手。
一般来说，系统功能从表拆分比较容易，因为业务流程基本都会围绕实体表运转，并关联多个实体进行交互。主要的拆分依据是：
数据实体职能只做最核心的一件事 业务流程归类按涉及实体进行归类，看能否分为多个阶段 由数据依赖交叉的频率决定是否把模块拆分，如果交互频繁可以合并 然后就是抽象的处理，越是底层的服务，越要注意抽象，主要为了减少变更，避免影响其他服务，因为很难确认底层的变更对上游系统的影响范围。
几个常见的抽象思路：
被动抽象法 如果两个或多个服务使用同一个业务逻辑，就把这个业务逻辑抽象成公共服务。适合代码量不大、维护人员很少、处于探索阶段的系统。
同层级之间的模块是禁止相互调用的。如果调用了，就需要将两个服务抽象成公共服务，让上层对两个服务进行聚合，目的是为了让系统结构从上到下是一个倒置的树形， 保证不会出现引用交叉循环的情况
动态辅助表 适用于规模稍微大一点的团队或系统。
如果一个表被多个开发组使用，而不同的业务特性也不同，可以在主表内存储 type，然后再根据 type 去对应不同的辅助表。例如：普通商品保存在表 order 和表 order_product_extra 中，定制类商品保存在 order_customize_extra 中。
强制标准接口 底层服务只做标准的服务，业务的个性部分都由业务自己完成。算是比较常见的做法。
总结 先从上到下做业务流程梳理，将流程归类聚合；然后从不同的领域聚合中找出交互所需主要实体，根据流程中主要实体之间的数据依赖程度决定是否拆分（从下到上看）； 把不同的实体和动作拆分成多个模块后，再根据业务流程归类，划分出最终的模块（最终汇总）。
极客时间的课，总结往往特别精华……</description>
    </item>
    
    <item>
      <title>Raft一致性算法论文中文翻译</title>
      <link>https://dabao-zhao.github.io/posts/raft%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/</link>
      <pubDate>Thu, 10 Aug 2023 16:36:39 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/raft%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91/</guid>
      <description>对 https://github.com/maemual/raft-zh_cn 的一个副本，防止丢失
寻找一种易于理解的一致性算法（扩展版） 寻找一种易于理解的一致性算法（扩展版） 摘要 1 介绍 2 复制状态机 3 Paxos 算法的问题 4 为了可理解性的设计 5 Raft 一致性算法 5.1 Raft 基础 5.2 领导人选举 5.3 日志复制 5.4 安全性 5.4.1 选举限制 5.4.2 提交之前任期内的日志条目 5.4.3 安全性论证 5.5 跟随者和候选人崩溃 5.6 时间和可用性 6 集群成员变化 7 日志压缩 8 客户端交互 9 算法实现和评估 9.1 可理解性 9.2 正确性 9.3 性能 10 相关工作 11 结论 12 感谢 参考 摘要 Raft 是一种为了管理复制日志的一致性算法。它提供了和 Paxos 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统。为了提升可理解性，Raft 将一致性算法分解成了几个关键模块，例如领导人选举、日志复制和安全性。同时它通过实施一个更强的一致性来减少需要考虑的状态的数量。一项用户研究的结果表明，对于学生而言，Raft 算法比 Paxos 算法更加容易学习。Raft 算法还包括一个新的机制来允许集群成员的动态改变，它利用重叠的大多数来保证安全性。</description>
    </item>
    
    <item>
      <title>共识Raft：如何保证多机房数据的一致性</title>
      <link>https://dabao-zhao.github.io/posts/%E5%85%B1%E8%AF%86raft%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%A4%9A%E6%9C%BA%E6%88%BF%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/</link>
      <pubDate>Wed, 09 Aug 2023 11:20:37 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%85%B1%E8%AF%86raft%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E5%A4%9A%E6%9C%BA%E6%88%BF%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/</guid>
      <description>答：基于 Raft 协议的分布式数据服务
参考资料 raft 算法的动画展示 官网 中文论文 </description>
    </item>
    
    <item>
      <title>同城双活：如何实现机房之间的数据同步</title>
      <link>https://dabao-zhao.github.io/posts/%E5%90%8C%E5%9F%8E%E5%8F%8C%E6%B4%BB%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%9C%BA%E6%88%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/</link>
      <pubDate>Wed, 09 Aug 2023 10:58:10 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E5%90%8C%E5%9F%8E%E5%8F%8C%E6%B4%BB%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%9C%BA%E6%88%BF%E4%B9%8B%E9%97%B4%E7%9A%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/</guid>
      <description>答：otter
TODO 后面再找一个 otter 的教程吧
常见的网络延迟参考 指一次 RTT 请求
同机房服务器：0.1 ms 同城服务器（100 公里以内）：1ms 北京到上海：38ms 北京到广州：53ms </description>
    </item>
    
    <item>
      <title>Token：如何降低用户身份鉴权的流量压力？</title>
      <link>https://dabao-zhao.github.io/posts/token%E5%A6%82%E4%BD%95%E9%99%8D%E4%BD%8E%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E9%89%B4%E6%9D%83%E7%9A%84%E6%B5%81%E9%87%8F%E5%8E%8B%E5%8A%9B/</link>
      <pubDate>Tue, 08 Aug 2023 17:21:24 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/token%E5%A6%82%E4%BD%95%E9%99%8D%E4%BD%8E%E7%94%A8%E6%88%B7%E8%BA%AB%E4%BB%BD%E9%89%B4%E6%9D%83%E7%9A%84%E6%B5%81%E9%87%8F%E5%8E%8B%E5%8A%9B/</guid>
      <description>答：用 JWT
TODO 后面再找一个 JWT 的教程吧</description>
    </item>
    
    <item>
      <title>缓存一致：读多写少时，如何解决数据更新缓存不同步</title>
      <link>https://dabao-zhao.github.io/posts/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E8%AF%BB%E5%A4%9A%E5%86%99%E5%B0%91%E6%97%B6%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0%E7%BC%93%E5%AD%98%E4%B8%8D%E5%90%8C%E6%AD%A5/</link>
      <pubDate>Tue, 08 Aug 2023 15:22:18 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E8%AF%BB%E5%A4%9A%E5%86%99%E5%B0%91%E6%97%B6%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E6%9B%B4%E6%96%B0%E7%BC%93%E5%AD%98%E4%B8%8D%E5%90%8C%E6%AD%A5/</guid>
      <description>主要讲临时缓存和长期缓存，还有如何保障缓存数据的一致性
临时缓存和长期缓存 这里的临时缓存是指 TTL 比较短的缓存，长期缓存指 TTL 比较长的缓存而不是常驻的缓存
缓存更新办法 一般情况下简单数据可以在更新的时候清理缓存，等下次读取时刷新缓存，防止并发修改导致临时数据进入缓存。
复杂数据的更新可以使用 Maxwell 和 Canal 对 MySQL 的更新进行监控，这样变更信息会推送到 Kafka，然后脚本就可以进行消费去更新缓存
还介绍了版本号更新缓存和脚本遍历更新缓存，个人认为用处不大
缓存穿透问题 文里这块长期缓存和缓存穿透放一块了，其实容易让人认为长期缓存解决了缓存穿透的问题，实际上两者的关系不大。
百度百科对缓存穿透的解释是：缓存穿透是指查询一个一定不存在的数据，由于缓存是不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，进而给数据库带来压力。
临时缓存 + 长期热缓存的实现 // 尝试从缓存中直接获取用户信息 userinfo, err := Redis.Get(&amp;#34;user_info_9527&amp;#34;) if err != nil { return nil, err } //缓存命中找到，直接返回用户信息 if userinfo != nil { return userinfo, nil } //set 检测当前是否是热数据 //之所以没有使用Bloom Filter是因为有概率碰撞不准 //如果key数量超过千个，建议还是用Bloom Filter //这个判断也可以放在业务逻辑代码中，用配置同步做 isHotKey, err := Redis.SISMEMBER(&amp;#34;hot_key&amp;#34;, &amp;#34;user_info_9527&amp;#34;) if err != nil { return nil, err } //如果是热key if isHotKey { //没有找到就认为数据不存在 //可能是被删除了 return &amp;#34;&amp;#34;, nil } //没有命中缓存，并且没被标注是热点，被认为是临时缓存，那么从数据库中获取 //设置更新锁set user_info_9527_lock nx ex 5 //防止多个线程同时并发查询数据库导致数据库压力过大 lock, err := Redis.</description>
    </item>
    
    <item>
      <title>结构梳理：大并发下，你的数据库表可能成为性能隐患</title>
      <link>https://dabao-zhao.github.io/posts/%E7%BB%93%E6%9E%84%E6%A2%B3%E7%90%86%E5%A4%A7%E5%B9%B6%E5%8F%91%E4%B8%8B%E4%BD%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E5%8F%AF%E8%83%BD%E6%88%90%E4%B8%BA%E6%80%A7%E8%83%BD%E9%9A%90%E6%82%A3/</link>
      <pubDate>Tue, 08 Aug 2023 14:12:36 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/%E7%BB%93%E6%9E%84%E6%A2%B3%E7%90%86%E5%A4%A7%E5%B9%B6%E5%8F%91%E4%B8%8B%E4%BD%A0%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A1%A8%E5%8F%AF%E8%83%BD%E6%88%90%E4%B8%BA%E6%80%A7%E8%83%BD%E9%9A%90%E6%82%A3/</guid>
      <description>主要以用户中心为例，讲解如何对读多写少的系统进行高并发优化。
将数据分为了四种类型：数据实体表，辅助查询表，实体关系表，历史数据表。然后根据不同类型讲述了不同的缓存策略。
数据实体表 每行数据代表一个实体，会存在一个唯一 ID。 需要根据业务需要对表进行精简，只保留所需字段，其他字段可以纵向拆分到辅助查询表。 数据小了之后，能减少 B+Tree 的层次，查询和传输也会更快。 对于这个表来说，一般会使用 前缀_ID 作为 key 进行缓存。对于一些组合条件或者对数据会做计算的一般会采用定期更新缓存的办法或者单独分出一个从库。
辅助查询表 目的是拆分出使用频率不高的字段。这里主要是提醒要定期对数据进行整理核对来保障数据的同步和完整。
实体关系表 三种关系：1:n、n:1、m:n
尽量减少 m:n 出现的可能性，如果出现要额外用一个关系表来维护关联关系。关系表在高并发系统中一般会降低一致性 要求来满足高并发的情况。主要可能会出现多级依赖。
历史数据表 一般不会去做缓存，数据量大而且增长量也大
判断是否适合缓存的核心思路 能够通过 ID 快速匹配的实体，以及通过关系快速查询的数据，适合放在长期缓存当中 通过组合条件筛选统计的数据，也可以放到临时缓存，但是更新有延迟 数据增长量大或者跟设计初衷不一样的表数据，这种不适合、也不建议去做做缓存 总结 核心就是要对数据进行归类，然后再根据归类做对应的处理</description>
    </item>
    
    <item>
      <title>Linux 下安装和升级 Go</title>
      <link>https://dabao-zhao.github.io/posts/linux%E4%B8%8B%E5%AE%89%E8%A3%85%E5%92%8C%E5%8D%87%E7%BA%A7go/</link>
      <pubDate>Tue, 08 Aug 2023 09:56:13 +0800</pubDate>
      
      <guid>https://dabao-zhao.github.io/posts/linux%E4%B8%8B%E5%AE%89%E8%A3%85%E5%92%8C%E5%8D%87%E7%BA%A7go/</guid>
      <description>安装 wget https://golang.google.cn/dl/go1.20.7.linux-amd64.tar.gz rm -rf /usr/local/go tar -C /usr/local -xzf go1.20.7.linux-amd64.tar.gz 打开 ~/.bashrc 并输入
export GOROOT=/usr/local/go export GOPATH=$HOME/gowork export GOBIN=$GOPATH/bin export PATH=$GOPATH:$GOBIN:$GOROOT/bin:$PATH 保存并关闭文件，输入以下命令以使更改生效
source ~/.bashrc 升级 操作与安装一致</description>
    </item>
    
  </channel>
</rss>
